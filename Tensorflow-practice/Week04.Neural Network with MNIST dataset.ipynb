{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MNIST 데이터 불러오기\n",
    "\n",
    "1만 개의 MNIST 훈련용/테스트용 데이터셋을 이전에 저장했던 바이너리 파일에서 불러오겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "mnist_small = pickle.load(open(r'D:/Dev/Python-ML/data/mnist/mnist_small.pickle', 'rb'))\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = mnist_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 데이터셋을 사용하고 싶다면 아래와 같은 코드를 입력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('D:/Dev/Python-ML/data/mnist/', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 분류 방법\n",
    "\n",
    "이산적인 값으로 레이블된 데이터를 분류하는 방법에는 여러가지가 있습니다.\n",
    "\n",
    "- 서포트 벡터 머신\n",
    "- 랜덤 포레스트\n",
    "- 로지스틱 회귀\n",
    "- 신경망\n",
    "- 컨벌루젼 신경망\n",
    "- RNN\n",
    "\n",
    "\n",
    "분류 알고리즘을 사용하는 환경에도 다양한 옵션이 있습니다.\n",
    "\n",
    "- 파이썬 직접 코딩\n",
    "- scikit-learn\n",
    "- Tensorflow\n",
    "- Keras\n",
    "- Pytorch\n",
    "\n",
    "물론 파이썬이 아닌 다른 언어에서 사용 가능한 모듈도 다양합니다.\n",
    "\n",
    "신경망을 직접 코딩하는 것은 그리 쉽지는 않지만 그리 어렵지도 않습니다. 신경망의 퍼셉트론과 훈련 알고리즘을 이애해야하면 코딩할 수 있습니다.  컨번루젼 신경망을 코딩하려면 컨벌루젼 층의 다차원 배열의 변환에 익숙해야 합니다.\n",
    "\n",
    "사이킷런에서는 Depp neural network 를 제대로 구현할 수는 없고, 다층 퍼센트론만 구현할 수 있습니다.\n",
    "\n",
    "텐서플로우는 배우기 쉬운 머신러닝 언어는 아닙니다.  하지만 신경망 이론을 이해하고 있다면 다른 사람의 코드를 보고 쉽게 응용할 수 있습니다.\n",
    "\n",
    "Keras, Pytorch 에서는 더 간편하게 신경망을 구현할 수 있습니다.\n",
    "\n",
    "신경망을 이해하고 Tensorflow, Keras, Pytorch 중에서 하나를 다룰 줄 알면 (텐서플로우만은 쉽다고 까지는 말을 못하지만) 나머지는 그리 어렵지 않게 배울 수 있습니다.\n",
    "\n",
    "머신러닝을 처음 배우는 단계이고 현업에서 사용하는 것이 아니라면 신경망 구성은 keras 로 익히고 나머지 머신러닝 시스템에는 사이킷런을 사용하는 것을 권합니다.\n",
    "\n",
    "먼저 여기서는 신경망 표현이 간단한 keras 를 사용해서 신경망 구성에 대해서 설명드리겠습니다.\n",
    "\n",
    "keras 는 pip 로 인스톨 합니다.   \n",
    "\\>>> pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임포트하는 모듈도 많은데 간단하게 설명하겠습니다.\n",
    "\n",
    "Sequential 은 신경망 레이어를 담는 그릇입니다.  Sequential 의 인스턴슬르 만들고 레이어를 연속해서 추가합니다.\n",
    "\n",
    "추가하는 레이어가 많을 수록 깊은 즉 deep neural network 가 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=64, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 Dense 레이어를 추가했습니다. 신경망의 하나의 은닉층을 표현합니다.  \n",
    "\n",
    "MNIST 의 입력 데이터의 크기인 784 를 inpu_dim 에 넣어줍니다.\n",
    "\n",
    "첫 번째 은닉층의 유닛(노드)는 64개 입니다.  입력 유닛의 크기인 784에 비해서 상당히 작은 값이지만 우선 이렇게 훈련시켜 보겠습니다.\n",
    "\n",
    "그 다음 층에 신호를 전달하기 전에 신호에 ReLU 함수를 적용시킵니다.    신경망에서는 활성함수로 sigmoid 대신에 ReLU 함수를 사용합니다.\n",
    "\n",
    "이 경우에는 층이 깊지 않으므로 sigmoid 함수를 사용해도 전혀 문제가 없습니다.\n",
    "\n",
    "그 다음 은닉층은 없고 바로 출력층입니다.  출력에서 예측해야하는 종류는 0부터 9까지의 10 종류의 숫자 입니다.  출력층의 유닛은 10으로 정합니다.\n",
    "\n",
    "출력층에서 나온 결과를 softmax 함수를 통과시키면 각 레이블별 확률로 해석할 수 있는 값이 계산됩니다.\n",
    "\n",
    "\n",
    "그 값을 중에서 가장 큰 값을 가지는 노드에 해당하는 레이블이 예측값이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 렇게 만든 모델에 대해서 추가 옵션을 정해서 컴파일을 합니다.\n",
    "\n",
    "loss 함수는 당연히 cross entropy 를 사용하고\n",
    "\n",
    "optimizer 에서는 스토캐스틱 gradient descent 방법을 사용합니다.\n",
    "\n",
    "출력 결과물은 'accuracy' 정확도만으로 설정했습니다.\n",
    "\n",
    "아래는 실제 데이터로 훈련시키는 과정입니다.\n",
    "\n",
    "전체 데이터에 대해서 총 10회 반복한고 한 번에 훈련시키는 배치의 크기는 100 입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 1.9790 - acc: 0.387 - 1s - loss: 1.9722 - acc: 0.3912     \n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 1s - loss: 1.3584 - acc: 0.7125     \n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 1s - loss: 0.9999 - acc: 0.7848     \n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 1s - loss: 0.8091 - acc: 0.8205     \n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 1s - loss: 0.6963 - acc: 0.8390     \n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 1s - loss: 0.6226 - acc: 0.8514     \n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 1s - loss: 0.5703 - acc: 0.8594     \n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 1s - loss: 0.5315 - acc: 0.8672     \n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 1s - loss: 0.5006 - acc: 0.8727     \n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 1s - loss: 0.4762 - acc: 0.8788     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1263ee10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련이 진행될 수록 loss (또는 cost) 가 줄어드는 것을 확인할 수 있습니다.  정확도는 점점 좋아져서  훈련데이터의 정확도가 87.88% 입니다.\n",
    "\n",
    "모델 성능에 대한 평가에는 테스트셋을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9216/10000 [==========================>...] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.45433075084686281, 0.88219999999999998]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_and_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트셋에 대해서 모델을 평가하니 정확도가 88.21% 가 나왔습니다.\n",
    "\n",
    "이전의 SVM 에서는 92.36% 가 나왔으니 지금은 형편없는 신경망입니다.\n",
    "\n",
    "신경망을 더 깊고 넖게 만들어서 훈련시켜보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# 입력층 + 첫 번쨰 은닉층\n",
    "model.add(Dense(units=512, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# 은닉층 \n",
    "model.add(Dense(units=512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=256))\n",
    "model.add(Activation('relu'))\n",
    "# 오버피팅을 방지하기 위한 층\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# 출력층\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "은닉층을 추가해주는 방법은 unit 의 갯수를 정해서 model.add(Dense(units=512)) 와 같이 추가한 후에 \n",
    "\n",
    "활성화층인  model.add(Activation('relu')) 를 추가해주면 됩니다.  \n",
    "\n",
    "10 개 정도를 사용할 수도 있고 100 개 이상을 사용할 수도 있습니다.\n",
    "\n",
    "오버피팅을 방지하기 위해서 Drop out 층을 넣을 수도 있습니다.\n",
    "\n",
    "이번에는 optimizer 에 추가 파라미터도 지정해봤습니다.  learning_rate 는 0.01 이고 momentum 은 0.9입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True),\n",
    "              metrics=[metrics.mae, metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 8s - loss: 0.0496 - mean_absolute_error: 0.0069 - categorical_accuracy: 0.9889     \n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 7s - loss: 0.0413 - mean_absolute_error: 0.0059 - categorical_accuracy: 0.9913     \n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 8s - loss: 0.0329 - mean_absolute_error: 0.0050 - categorical_accuracy: 0.9942     \n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 8s - loss: 0.0286 - mean_absolute_error: 0.0045 - categorical_accuracy: 0.9953     \n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 8s - loss: 0.0209 - mean_absolute_error: 0.0034 - categorical_accuracy: 0.9972     \n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 7s - loss: 0.0173 - mean_absolute_error: 0.0030 - categorical_accuracy: 0.9983     \n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 8s - loss: 0.0143 - mean_absolute_error: 0.0025 - categorical_accuracy: 0.9989     \n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 7s - loss: 0.0113 - mean_absolute_error: 0.0021 - categorical_accuracy: 0.9997     \n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 7s - loss: 0.0094 - mean_absolute_error: 0.0017 - categorical_accuracy: 0.9998     \n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 8s - loss: 0.0079 - mean_absolute_error: 0.0015 - categorical_accuracy: 0.9999     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x141d7eb8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9984/10000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.16518993893722073, 0.010180272687831893, 0.95830000000000004]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_and_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조금 더 갚은 신경망을 훈련시킨 결과 테스트셋에서 95.83% 의 정확도를 얻었습니다.\n",
    "\n",
    "신경망의 파라미터와 옵션을 조정해서 실습을 해볼 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
